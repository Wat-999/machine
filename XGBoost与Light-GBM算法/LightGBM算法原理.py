#LightGBM算法是Boosting算法的新成员，由微软公司开发。它和XGBoost算法一样是对GBDT算法的高效实现，
#在原理上与GBDT算法和XGBoot算法类似，都采用损失函数的负梯度作为当前决策树的残差近似值，去拟合新的决策树
#与传统的机器学习算法相比，LightGBM算法具有的优势：训练效率更高，低内存使用，准确率高，支持并行化学习，可以处理大规模数据

#1基于leaf-wise的决策树生长策略
#大部分决策树算法的生长策略是leaf-wise生长策略，即同一层的叶子节点每次都一起分裂，但实际上一些叶子节点但分裂增益较低，这样分裂会增加不小的开销
#LightGBM算法使用的则是leaf-wise生长策略，每次在当前叶子节点找出分裂增益最大的叶子节点进行分裂，而不是所有节点都进行分裂，这样可以提高精准度
#但是leaf-wise策略在样本量较小时容易造成过拟合，LightGBM算法可以通过参数max_depth限制树的深度来防止过拟合


#2直方图算法
#直方图分为频数直方图和频率直方图，横坐标为相关数据，纵坐标为该数据出现的频数或频率
#直方图算法又称histogram算法，简单来说，就是先对特征值进行装箱处理，将连续的浮点特征值离散化成k个整数，形成一个个箱体(bins)，
#同时构造一个宽度为k的直方图，在遍历数据时，以离散化后的值作为索引在直方图中累积统计量(因此这里是频数直方图)，
#遍历一次数据后，直方图累积了需要的统计量，再根据直方图的离散值遍历寻找最优分割点。
#对于连续特征来说，装箱处理就是特征工程中的离散化，例如[0,10)区间的值都赋值为0，[10,20)区间的值都赋值为1等，
#这样就可以把众多的数值划分到有限的分箱中，LightGBM算法中默认的分箱数(bins)为256
#举例来说，现在有10000个客户，也就有10000个身高数据，将身高分箱为256份后(例如，身高180～180.2cm的所有客户都分箱为数字200，
#那么数字200对应的频数就是100）这样在节点分裂时，就不需要按照预排序算法对每一个特征都计算10000遍(样本总数），而只需要计算256遍（分箱数），大大加快量训练速度
#对于分类特征来说，则是将每一种取值放入一个分箱（bin），且当取值的个数大于最大分箱数时，会忽略那些很少出现的分类值。例如，10000个客户的国籍数据，
#遍可以按国家名称进行分箱，如果超过最大分箱数（如256），那么很少出现的国家就会被忽略


#并行学习
#LightGBM算法支持特征并行和数据并行两种方式。传统的特征并行的主要思想是在并行化决策树中寻找最佳切分点，在数据量大时难以加速，同时需要对切割分结果进行通信整合。
#LightGBM算法在本地保存全部数据，这样就没有了机器间通信所需的开销。此外，传统的数据并行是构建本地直方图，然后进行整合，在全局直方图中寻找最佳切分点。
#LightGBM算法使用分散规约(reduce scatter),将直方图合并的任务分给不同的机器，降低通信和计算的开销，并利用直方图做加速训练，进一步减少开销。
#除上述原理，LightGBM算法还包含一些重要的算法思想，如单边梯度采样GOSS算法和互斥绑定EFB算法，
#在GOSS算法中，梯度更大的样本点在计算信息增益时会发挥更重要的作用，当对样本进行下采样时保留这些梯度较大的样本点，并随机去掉梯度小的样本点，
#EFB算法则将互斥特征绑在一起减少特征维度

#LightGBM分类模型
from lightgbm import LGBMClassifier
x = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]
y = [0, 0, 0, 1, 1]
model = LGBMClassifier()
model.fit(x, y)
print(model.predict([[5, 5]]))


#LightGBM回归模型
from lightgbm import LGBMRegressor
x = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]
y = [1, 2, 3, 4, 5]
model = LGBMRegressor()
model.fit(x, y)
print(model.predict([[5, 5]]))