#决策树模型既能做分类分析(即预测分类变量值),又能做回归分析(即预测连续变量值),对应的模型分别为分类决策树模型(Decision TreeClassifier）
#和回归决策树模型（Decision TreeRegressor）
#决策树模型的建树依据主要用到的是基尼系数的概念。基尼系数（gini ）用于计算一个系统中的失序现象，即系统的混乱程度。
#基尼系数越高，系统的混乱程度就越高2，建立决策树模型的目的就是降低系统的混乱程度，从而得到合适的数据分类效果。
#采用基尼系数进行运算的决策树也称为CART决策树，
# 除了基尼系数，还有另一种衡量系统混乱程度的经典手段-信息熵。在搭建决策树模型时，信息熵的作用和基尼系数是基本一致的，都可以帮助我们合理的划分节点。

from sklearn.tree import DecisionTreeClassifier  #引入分类决策树模型DecisionTreeClassifier
x = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]    #x是特征变量，共有5个训练数据，每个数据有2个特征，如数据[1,2],它的第一个特征的数值为1，第二个特征的数值为2
y = [1, 0, 0, 1, 1]     #y是目标变量，共有2个类别——0和1
model = DecisionTreeClassifier(random_state=0)    #引入模型设置随机状态参数random_state=0为0，这里0没有意义可以换成其他数字，它是一个种子参数，可使每次运行结果一致
model.fit(x, y)   #用fit（）函数训练模型
print(model.predict([[5, 5]]))    #用predict（）函数进行预测
