#假设目标变量Y代表信用卡额度，特征变量X1代表月收入，回归模型：Y = X1 + 500
#此时特征变量X1的系数为1，这表明收入增加1个单位时，信用卡额度也会增加1个单位。
#如果在该回归模型中加入另一个特征变量X2，其样本数据恰好与X1完全相同，那么该回归模型有可能会变成如下形式Y = 0.5X1 +0.5X2 +500
#特征变量X1的系数从1变成0.5，这表明收入增加1个单位时，信用卡额度只会增加0.5个单位，与实际情况不符合，某种程度上削弱另月收入的重要性
#因此多重共线性会对回归模型的预测结果产生不利的影响
#总体来说，在实际应用中，多重共线性会带来如下不利影响
#线性回归估计式变得不确定或不精确
#线性回归方差变得很大，标准误差增大
#当多重共线严重时，甚至可能使估计的回归系数符号相反，得出错误的结论
#削弱特征变量的特征重要性

#1数据读取
import pandas as pd
df = pd.read_excel('/Users/macbookair/Desktop/数据分析/书本配套资料及电子书/python/Python大数据分析与机器学习/源代码汇总-2020-12-16/第11章 特征工程之数据预处理/源代码汇总_PyCharm格式/数据.xlsx')
print(df.head())
#2对数据集划分特征变量和目标变量
X = df.drop(columns='Y')
Y = df['Y']

#3相关系数判断
print(X.corr())  #corr()函数计算不同变量的相关系数（用的是皮尔逊相关系数）
#结果解读，其中第i行第j列的内容表示第i个特征变量和第j个特征变量的相关系数
#相关系数判断使用起来非常简单，结论也比较清晰，不过有一个缺点：简单相关系数只是多重共线性的充分条件，不是必要条件。
#在有多个特征变量时，相关系数较小的特征变量间也可能存在较严重的多重共线性。
#为了更严谨，实战中还经常用到方差膨胀系数法（VIF检验）：VIF=1/(1-R^2)
#其中VIF是衡量自变量Xi是否与其他自变量具有多重共线性的方差膨胀系数，R^2是将自变量Xi作为因变量，其它自变量作为特征变量时回归的可决系数
#R-squared值，是用来衡量拟合程度的。R^2越大，VIF就越大，表示自变量Xi与其他自变量间的多重共线性越严重。
#一般认为VIF<10时，该自变量与其余自变量之间不存在多重共线性；
#当10<=VFI<100时，存在较强的多重共线性
#当VIF>=100时存在严重的共线性。
#例如，对于只有两个特征变量的回归方程y=f(X1,X2)，若X1=X2，根据R-squared值的知识，
#X1=X2即完全线性拟合，那么衡量拟合程度的可决系数R2=1，则方差膨胀系数VFI=1/(1-1)=+∞，大于100，即存在严重的多重共线性

from statsmodels.stats.outliers_influence import variance_inflation_factor
#vif = [variance_inflation_factor(X.values, X.columns.get_loc(i))for i in X.columns]  #等同下面的写法
#X.columns.get_loc(i))返回的是指定列的序号数字，如第一列返回的就是数字0
#通过for循环依次求得每个特征变量的方差膨胀系数并将结果放入列表中
vif = []
for i in X.columns:  #i对应每一列的列名
    vif.append(variance_inflation_factor(X.values, X.columns.get_loc(i)))

    print(vif)

#结果解读因为特征变量X2是X1的2倍(数据集中），所以使用X1对X2和X3，和使用X2对X1和X3回归时所得的方差膨胀系数会很大，
#从结果可以看出，前2个vif值均大于100，暗示多重共线性十分严重，应该删掉X1或X2

#删掉X2在进行依次回归和VIF检验
#重新划分特征变量和目标变量
X = df[['X1', 'X3']]
Y = df['Y']
vif = []
for i in X.columns:  #i对应每一列的列名
    vif.append(variance_inflation_factor(X.values, X.columns.get_loc(i)))

    print(vif)
#可以看到，此时两个特征变量的方差膨胀系数都小于10，说明它们之间不存在多重共线性。
#总结来说，对于线性回归模型和逻辑回归模型等以线性方程表达式为基础的机器学习模型，需要注意多重共线性的影响。如果存在多重共线性，
#则需要进行相应处理，如删去某个引起多重共线性的特征变量






